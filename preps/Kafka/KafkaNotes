Kafka Topics
	--> a perticular stream of data
	--> like table in a database(without all the constrains)
	--> kafka cluster
		--> logs
		--> purchases
		--> tweets
		--> truck_gps ..etc
	--> we can have as many topics as we want
	--> topic was identified by its name
	--> supports any kind of message format
	--> sequence of message called as data stream in topic
	--> we cant query the topics insted we use produces to send data and consumers to read data
	--> topics are splits in partitions
		--> messages with in partitions are orders(follows order)
	 	--> each message with in partitions gets an increment id called as offset
	 	--> offset only have meaning for a specific partitioin
	 	--> offsets are not reused even if previous message been deleted
	 	--> order is only guaranteed only with in a partition
	--> topics are immutable
	--> data kept in kafka in limited time default was 1 week can be configurable
	--> data is assigned randomly to partition unless akey provided

Producers and Message key
	--> producers write data to topics (which are made of partitions)
	--> producer knows which partition to write to (and which kafka broker has it)
	--> In case of kafka broker failure, produces will autometically recover (this load is balanced to many brokers thanks to no of partitions)
	--> producers can choose to send a key with the message(string,number,binary ..etc) its optional
	--> if key is null then the data is sent round robin (part 0,then 1 ,then 2 ..)
	--> if key is not null then all messages for that key will always go to the same parttion(hashing)
	--> A key are typically sent if you need message ordering for specific field
	--> Kafka message created by the produce have below components
		--> Key -binary (can be null)
		--> Value - binary (can be null)
		--> Compression Type (none,gzip,snappy..etc)
		--> Headers (optional)
			--> key-value pairs
		--> Partition + Offset
		--> Timestamp(system or user set)
	--> kafka message serializer
		--> kakfka only accepts bytes as an input from produces and sends bytes out as an output to consumers
		--> message serialization means transforming objects/data into bytes
		--> they are used on the value and the key
		--> Common serializers
			--> String (incl. JSON)
			--> Int,Float
			--> Avro, Protobuf ..etc
	--> the default partitioner, the keys are hashed using the murmur2algorith

Consumers and deserialization
	--> Consumers read data from a topic(identified by name) - pull model
	--> Consumers automatically know which broker to read from
	--> in case of broker failures, consumers know how to recover
	--> Data is read in order from low to high offset within each partitions
	--> consumers will deserialize objects/data
	--> serialization/deserialization type must not change during a topic lifecycle(create new topic insted)
	--> Consumer groups
		--> all the consumers in an application read data as consumer groups
		--> each consumer within group reads from exclusive partitions
		--> if we have more consumers than partitions then some consumers will be inactive
		--> it is acceptable to have multiple consumer groups on the same topic (each goup may have different fuctinality like notification,location track ..etc)
		--> to create distinct consumer groups,use the consumer property group.id
	--> we can have multiple consumers on one topic
	--> Consumer offsets
		--> kafka stores the offsets at which a consumer group has been reading
		--> the offsets committed are in kafka topic named __consumer_offsets
		--> when a consumers in a group has processed data received from kafka,it should be periodically committing the offsets
		(kafka broker will write to __consumer_offsets, not the group itself)
		-->  if a consumer dies, it will be able to read back from where it left off thanks to the commited consumer offsets
	--> delivery semantics
		--> by default, java consumers will automatically commit offsets(atleast once)
		--> thare are 3 delivery semantics if you choose to commit manually
			--> atlease once(usually preferred)
				--> offsets are commited after message processed
				--> if processing going wrong, the message will be read again. it result in duplicate processing
				so we should make shure that process should be idempotent(i.e., processing again should not imapact system)
			--> At most once
				--> offsets are commited assoon as message received
				--> if the process gone wrong, some message can be lost
			--> Exactly once
				--> for kafka : kafka workflows : use the transactional API(easy with kafka streams API)
				--> for kafka : external system workflows : use an idempotent consumer
Brokers and Topics
	--> A kafka cluster is composed of multiple brokers(servers)
	--> Each broker is identified with its ID (integer)
	--> Each broker contains certain topic partitions
	--> After connecting to any broker (called bootstrap broker), you will be connected to the entire cluster
	(kafka clients have smart machanics for that)
	--> A good number to get started is 3 brokers, but some big clusters have over 100 brokers
	--> Topic partitions will spread over the kafka brokers (horizantal scaling)
	--> every broker called as bootstrap server. we need to connect to one broker and kafka clients will know how to connected to entire cluster
	--> each brokers knows the meta data of all broker topics and its partitions

Topic Replication
	--> topics should have replication factor > 1(usually btw 2 and 3)
	--> if any broker down another broker serve the data
	--> at any time only one broker can be a leader for a given partition
	--> Producers can only send data to the broker that is leader of partition
	--> the other brokers will replcate the data. so each partition has one leader and multiple ISR (insync replication)
	--> by default kafka producers can only write to the broker for partition and consumer will read the data from leader
	--> since kafka 2.4, it is possible to configure consumers to read from the closest replica. this may reduce latency and also decrease network cost if using cloud
Producer Acknowledments(acks)
	-->produces can chosse to receive acknowledgement of data writes
		--> acks=o : producers wonnt wait for ack (possible of data loss)
		--> acks=1 : will wait for leader ack (limited data loss) default from 1.0 to 2.8
		--> acks=all : leader+ replica acks (no data loss) default for 3.0+
			--> we can set howmany replica acks needed for us to ack back using prop min.insync.replicas
				--> if min.insync.replicas =1 ; only broker leader needs to success ack
				--> if 2 atleasr the broker leader and one replica need to ack
Topic Durability
	--> For a topic replication factor of 3,topic data durability can withstand 2 brokers loss
	--> as a rule, for replication factor of N, you can permanantly lose up to N-1 brokers still recover data

Zookeeper
	--> it manages brokers
	--> it helps in performing leader election for partitions
	--> sends notifications to kafka in case of changes (ex: new topic, broker dies. broker up..etc)
	--> till 2.X kafka cant work without zookeeper
	--> kafka 3.x can work without zookeper(KIP-500) - using kafka Raft insted
	--> kafka 4.x will not have zookeeper
	--> by design zookeper operates with odd number of servers (1,3,5..)
	--> Zk has leader (writes) the rest of the servers are followers(reads)
	--> ZK doesnot store consumers offsets with kafka >v0.10
	--> should we use zookeper?
		--> with kafka brokers?
			--> yes untill kafka 4.x 
		--> with kafka clients?
			--> over the time , the kafka clients and CLI have been migrated to leverage the brokers as a connection endpoint insted of zK
			--> Since kafka 0.10, consumers stores offset in kafka and ZK and must not connect to ZK as it is deprecated
			--> Since kafka 2.2, the kafka-topics.sh CLI command referance kafka brokers and not ZK for topic managment(creation,deletion..)
			and the ZK CLI arguments is deprecated
			--> All the APIs and commands that were previously leveraging ZK are migrated to use kafka insted, so that when clusters are 
			migrated to be without Zk, the change is invisible to clients
			--> ZK is also less secure than kafka, so ZK ports should only to be opend to allow traffic from brokers and not the clients
			--> so to be great modern-day kafka developer, never ever use ZK as a conficuration in ur kafka clientsand other programs that connect to kafka
Kafka KRaft
	--> in 2020, apache kafka project started to work to remove the zookeeper depeendency from it (KIP-500)
	--> ZK shows scaling issues when kafka clusters have >100000 partitions
	--> by removing ZK, apache kafka can
		--> scale to millions of partitions, and become easier to maintain and setup
		--> improve stability, makes it easier to monotor, support and administation
		--> Single security model for the whole system
		--> Single process to start with kafka
		--> Faster controller shutdown and recovery time
		--> kafka 3.x now implements the Raft protocol(KRAFT) in order to replace ZK. Not production ready

Installation guide:
--> https://www.conduktor.io/kafka/how-to-install-apache-kafka-on-mac

Kafka CLI : 
--> use --bootstrap-server option everywhere not --zookeeper 
--> right : kafka-topics --bootstrap-server localhost:9092 
--> Wrong : kafka-topics --zookeeper localhost:2181 
--> kafka-topics --bootstrap-server localhost:9092 --list : list the topics 
--> kafka-topics --bootstrap-server localhost:9092 --create --topic my_topic : creation 
--> kafka-topics --bootstrap-server localhost:9092 --create --topic my_topic --partitions 3 --replication-factor 2 : with partition and refactor --> it will thow error if we have less brokers -->
--> kafka-topics --bootstrap-server localhost:9092 --describe --topic my_topic  : details of topic
--> kafka-topics --bootstrap-server localhost:9092 --delete  --topic my_topic : if winown it may crash
--> kafka-console-producer bootstrap-server localhost:9092 --topic my_topic : to brodue
	> entrt messages..etx
	> ctrl+c for terminate
--> kafka-topics --bootstrap-server localhost:9092 --delete  --topic my_topic : if winown it may crash
--> kafka-console-producer bootstrap-server localhost:9092 --topic my_topic --producer-property acks=all : set acks
	--> we can run same command even there is no existing topic it will create new topic for us
--> kafka-console-producer bootstrap-server localhost:9092 --topic my_topic  --property parse.key = true 
	--property key.separatoor=:   (for key value pair)
	> my key : my value
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic : empty result as the pointer willl be on last
	--> open new console and proce mesage so we can see same message can be seen in consumer terminal
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic --from-beginning : to read all messages
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic --from-beginning --formatter kafka.tools.DefaultMessageFormatter
	--property print.timestamp=true --property print.key=true --property print.value=true : to view in key-value pair
	if no key it will be null
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic --group my_consumer_group : to group consumers
	--> we can run same cod in different terminal to vire message distribution
--> kafka-consumer-group -bs-srv lh:9092 --group my_group --reset-offsets --to-erliest --topic my_topic
	--execute : to resetting offset till the beggining
--> kafka-consumer-group -bs-srv lh:9092 --group my_group --reset-offsets --shift-by -2 --topic my_topic
	--execute : to resetting offset to desired shift

Kafka_Java
	--> https://www.conduktor.io/kafka/kafka-sdk-list for java sdk download
	--> maven dependencies 
		--> java 11
		--> kafka-clients
		--> slf4j api
		--> slf4j simple better to keep both slfj same version
	--> producer
		--> create producer configuration
			  	Properties props= new Properties();
        		props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"127.0.0.1:9092");
        		props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        		props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());

		--> create the producer
			    KafkaProducer<String,String> produder = new KafkaProducer<String, String>(props);

  		--> Producer Record
        		ProducerRecord record = new ProducerRecord("demo","hi how are you");
        --> send data (async)
        		produder.send(record);
        --> send data (sync)
       		 produder.flush();
        --> closing
        	produder.close();
    --> Producer with call back : callback method will be called for each succcessfull send or exception

    	-->  while sending data
    		-->   produder.send(record, new Callback() {
            public void onCompletion(RecordMetadata recordMetadata, Exception e) {
                if(e== null)
                    logi.info("message send sucessfully");
                else
                    logi.info(e.getMessage());

            }
        });
    --> Producer with keys // partition will be preserved based key hash
    	--> ProducerRecord record = new ProducerRecord("demo","key1",hi how are you");
    --> Consumer
    	--> config  
    		Properties props= new Properties();
        	props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"127.0.0.1:9092");
        	props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        	props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        	props.setProperty(ConsumerConfig.GROUP_ID_CONFIG,"my_group_id");
        	props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest"); //"none/earliest/latest"
        --> create consumer
        	KafkaConsumer<String,String> consumer = new KafkaConsumer<String, String>(props);
        --> Topic sunscription
        	consumer.subscribe(Collections.singletonList("my_Topic")); // for single topic
        	consumer.subscribe(Arrays.asList("my_Topic_list")); // for multiple topics
        --> poll/read data

	        --> while (true){
	            ConsumerRecords<String,String> records = consumer.poll(Duration.ofMillis(1000));
	            for (ConsumerRecord<String,String> record:records) {
	                logi.info(record.key());
	                logi.info(record.value());}}
	    --> Gracefull shutdown
	    	--> after consumer creation
	    		--> get a reference to the current thread
	    			--> // shutdown thread
			        //get current thread

			        final Thread mainThread = Thread.currentThread();
			        //adding shutdown hook
			        Runtime.getRuntime().addShutdownHook(new Thread() {
			            public void run() {
			                logi.debug("Detected a shutdown. lets by calling wakeup function");
			                consumer.wakeup();
			                try {
			                    mainThread.join();
			                } catch (InterruptedException e) {
			                    e.printStackTrace();
			                }
			            }
			        });

			        try {

			            //topic subscription

			            consumer.subscribe(Collections.singletonList("my_Topic")); // for single topic
			            consumer.subscribe(Arrays.asList("my_Topic_list")); // for multiple topics

			            //poll/read data

			            while (true) {
			                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
			                for (ConsumerRecord<String, String> record : records) {
			                    logi.info(record.key());
			                    logi.info(record.value());
			                }

			            }
			        } catch (WakeupException we) {
			            we.getMessage()
			        } catch (Exception e) {
			            logi.error(e.getMessage());
			        } finally {

			            consumer.close(); // this will also do auto commit offset
			        }
		--> Consumer Groups
			--> groups will be rebalancing on partitions on below statergies
				--> Eagar Rebalance (default)
					--> once new consumer adds. it will cancell existing membership and reasigning
					--> during short period of time, the entaire consumer group stop processing
					--> consumers dont necessarly get back the same partition previously assigned
				--> Cooperative rebalancing(incremental rebalance)
					--> reassignng a small subset of partions from one consumer to another
					--> can go through several iterations
					--> Avoids "stop-the-world" events where all consumers stop processing
					--> property partition.assignment.strategy
						--> rangeAssignor : assign partitions on per-topic basis(can cause imbalance)
						--> RoundRobin
						--> StickyAssignor : balanced like RR and then minimises partition movements when consumer join/leave the group
						--> CooperativeStickyAssignor : rebalance strategy is identical to sticky assignor but supports cooerative balance
						and therefore consumer can keep on consuming topic 
						--> RA and CSA are default
			--> if consumer leaves group and join back it will have new member ID. if we specify group.instance.id makes consumer static member
				if its in session.timeout.ms
	--> Producer Retries
		--> in case of transient failures,developers are expected to handle exceptions,otherwise the data will be lost.
		--> Examples of failures
			--> NOT_ENOUGH_REPLICAS (due to min.insync.replicas settings)
		--> there is a retries settings
			--> default to 0 for kafka <=2.0
			--> defaults to 2147483647 for kafka >=2.1
			--> the retry.backoff.ms setting by default 100ms
		--> Timeouts
			--> if retries>0, for ex. its tries all retries then its bound to time out
			--> since 2.1 you can set delivery.timeout.ms=1200000==2 min 
	--> Idempotent Producer
		--> in kafka >=0.11 you can define a idempotent producer which wont introduce duplicates on network error
		--> it is create to guarantee a stable and safe pipeline
		--> they are default since kafka 3.0 recommended to use them
		--> they comeup with 
			--> retries=Integer.MAX_VALUE
			--> max.in.flight.requests=1(kafka=0.11) where as 5 for >=1.0 for high performance
			--> acks= all
		--> we need to set producerProps.put(enable.idempotence",true)
	--> Safe Producer
		--> since 3.0 the producer is safe by default
			--> acks = all (-1)
			--> enable.idempotence = true
		--> with <=2.8 default as 1 and false
		--> key props to make produce safe
			--> acks=all
			--> min.insync.replicas = 2(broker/topic level)
			--> enable.idempotent=true (duplicates are not introduced due to n/w retries)
			--> delivery.timeout.ms=120000
			--> max.in.flight.requests.per.connection=5
				--> ensure max performace while keeping message ordering
	--> Message compression at producer level
		--> produces usually send data that text-based, ex json
		--> in this case,it is to importance to compress to improve performance
		--> compression can be enabled at producer level doesnt require any config change in brokers or consumers
		--> compression.type can de none(default).gzip,lz2,snappy,zstd(kafka 2.1)
		--> we can set at broker level(all topics) or topic-level
		--> compression.type=prodcer(default),the broker takes compressed batch from procuder client 
		and write directly to topic log file withour recompressing the data. it was recommanded
		--> compression.type=none all batches decompressed by the broker
		--> compression.type= lz4(for ex)
			--> if its matching produer setting,data is stored on disk as is
			--> if not batches are decompressed by broker and then recompress ussing compression algoritm specified
	--> linger.ms and batch.size
		--> by default, producers try to send records asap
			--> it will have upto max.in.flight.... =5, meaning up to 5 message batches being in flight
			--> after this, if more msgs must be sent while others are in flight,kafka is smart and will start batching them before the next batch send
		--> above two settings to influence the batching machanism
		--> linger.ms:default 0 how long to wait untill we send a batch. adding small number ex 5 helps add more msgs in that batch at expence of latency
		--> batch-size: default 16KB if batch is filled before linger.ms increase batch size
		       	props.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG,"snappy");
        		props.setProperty(ProducerConfig.LINGER_MS_CONFIG,"20");
        		props.setProperty(ProducerConfig.BATCH_SIZE_CONFIG,Integer.toString(32*1024));
    --> if the producer produces faster than the broker , the records will be buffered in memory. below are helpfull
    	--> max.block.ms = 60000
    	--> bugger.memory = 33554432(32 MB) the size of send buffer

    --> Consumers (read data from topic send it to opensearch)
    	--> Delivery Semantics
    		--> Atmost once : offsets are committed as soon as the message batch received.if process gone wrong it wont read again
    		--> At least Once(preffered) : offsets are commited after msg processed. if process gone wrong it will read again. can be duplicate processing
    			so we need to make sure process in idempotent
    		--> Exactly once : can be acieved for kafka => kafka workflows using transactional API
    			(easy with kafka Streams API). for kafka => sink workflows,use an idempotent consumer
    		-->  we can make comsumer process to make idempotent ase on the ID
    	--> Offset Commit strategies
    		--> (easy) enable.auto.commit = true & synchroneous processing of batches
    			--> in the java consumer API, offsets are regularly commites
    			--> enable at-least once reading senario by default
    			--> offsets are commited when you call .poll() and auto.commit.interval.ms has elapsed
    			--> make sure msgs are all successfully processed before you call .poll() again
    				if we dont,you will not be in at-lease-once reading scenario
    			--> in that(rare) case, you must diable auto commit, and most likely most processing to a separate thread and then from time-to-time call
    				.commitSync() or .commitAsync() with correct offsets manually(advanced)
    			--> default intervel 5000
    		--> (medium) enable.auto.commit = false and manual commit of offset
    		--> Offset reset behaviour
    			--> A consumer expected to read from log continuosly
    			--> but if your application has bug, consumer will go down
    			--> if kafka has retention of 7 days, your consumer is down for more than 7 offsets are invalid
    			--> auto.offset.reset 
    				--> latest : will read from end of log
    				--> earlier : from start
    				--> none : throw exception if no offset found
    			--> offsets can be lost if 
    				--> consumer hasnt read data in 1 day (kafka < 2.0) where as 7 days for >=2.0
    			--> can be controlled by broker settings offset.retention.minutes
    	--> Controlling Consumer liveliness
    		--> Consumers in a group talk to a consumer groups coordinator
    		--> to detect consumers that are down there is a heartbeat mechanism and a poll machanism
    		--> to avoid issues,consumers are encouraged to process data fast and poll ofen
    		--> heartbeat.interval.ms (default 3 sec) usually set to 1/3rd of session.time.out.ms
    		--> session.timeout.ms (default 45 sec kafka 3.0+ before 10 sec)
    		--> heartbeats are sent periodically to the broker
    		--> if no heartbeat is sent during that period, the consumer is considered 
    		--> set even lower to faster consumer rebalanes
    		--> max.poll.interval.ms(default 5 mins)
    			--> max amount of time btw two .poll() calls before declaring the consumer dead
    			--> this machanism is used to detect a data processiing issue with the consumer(consumer is stuck)
    			--> max.poll.records(default 500)
    				--> controls how many records to receive per poll request
    				--> increase if msg are very small and have a lot of available RAM
    			--> fetch.min.bytes (default 1)
    				--> controls how much data we want to pull at least on each request
    			--> fetch.max.wait.ms(default 500)
    				--> the max amount of time the broker will block before answering
    					the fetch request if there isnt sufficient data to immediately satisfy the requirement given by fetch.min.bytes
    	--> default consumer behaviour with partition leaders
    		--> by default will read from the leader broker for a partition
    		--> possibly higher latency (multiple data centre)+ high network chargers 
    		--> ex: Datacentre == Availability Zone(AZ) in AWS 
    		--> since 2.4 it is possible to configure consumers to read from the closest replica
    		--> broker settings(must ve 2.4+)
    			--> rack.id  config must be set to data centreID(ex AZ ID in AWS) : rack.id=usw2-az1
    			--> replica.selector.class must be set to org.apache.kafka.common.replica.RackAwareReplicaSelector
    		--> Consmer settings
    			--> client.rack to the data centre ID the consumer is launched on
    			 



-------------------------------------- dipesh notes ----------------------------------------------------------------------

1. Install microservices base locally in to your m2

To install Jar use following command
mvn install:install-file -Dfile=/Users/drane/Documents/projects/broadridge/microservices-base-template/target/microservices-base-0.0.2-SNAPSHOT.jar -DgroupId=com.altimetrik.base.service -DartifactId=microservices-base -Dversion=microservices-base-0.0.2-SNAPSHOT -Dpackaging=jar

To install Pom Use following command
mvn install:install-file -Dfile=/Users/drane/Documents/projects/broadridge/microservices-base-template/pom.xml -DgroupId=com.altimetrik.base.service -DartifactId=microservices-base -Dversion=microservices-base-0.0.2-SNAPSHOT -Dpackaging=pom


2. Run docker compose file (docker-compose.yml) provided on your local machine. Make sure you have docker desktop installed on your machine. Go to directory where docker compose file is stored and run the following command. -d option runs it in backround as daemon processes.

command: docker-compose up -d 

For first time since no docker image is pulled on your machine it will take some time (10-12 minutes) depending on your internet connectivity.
Wait for all service status changed from Creating to Started/Created.

3. This compose will install kafka broker, schema registry, postgres, mongodb, kafka connectors etc.
Wait for couple of minutes till kafka eastabilishes communication between its internal containers. You can check all containers running using command

command: docker ps 

Output will be something like this.

CONTAINER ID   IMAGE                                             COMMAND                  CREATED              STATUS              PORTS                                            NAMES
cb6a5e303fce   confluentinc/cp-enterprise-control-center:7.7.1   "/etc/confluent/dock…"   About a minute ago   Up About a minute   0.0.0.0:9021->9021/tcp                           control-center
e39e60f2b5ec   confluentinc/ksqldb-examples:7.7.1                "bash -c 'echo Waiti…"   About a minute ago   Up About a minute                                                    ksql-datagen
d664b6a9e41d   confluentinc/cp-ksqldb-cli:7.7.1                  "/bin/sh"                About a minute ago   Up About a minute                                                    ksqldb-cli
706efba1e305   confluentinc/cp-ksqldb-server:7.7.1               "/etc/confluent/dock…"   About a minute ago   Up About a minute   0.0.0.0:8088->8088/tcp                           ksqldb-server
c05778cf6e9d   cnfldemos/cp-server-connect-datagen:0.6.4-7.6.0   "/etc/confluent/dock…"   About a minute ago   Up About a minute   0.0.0.0:8083->8083/tcp, 9092/tcp                 connect
390534f9d8c1   confluentinc/cp-kafka-rest:7.7.1                  "/etc/confluent/dock…"   About a minute ago   Up About a minute   0.0.0.0:8082->8082/tcp                           rest-proxy
3de4e2334c4a   confluentinc/cp-schema-registry:7.7.1             "/etc/confluent/dock…"   About a minute ago   Up About a minute   0.0.0.0:8081->8081/tcp                           schema-registry
e2cef13303f9   mongo                                             "docker-entrypoint.s…"   About a minute ago   Up About a minute   0.0.0.0:27017->27017/tcp                         compose-mongo-1
9fcec7806793   mongo-express                                     "/sbin/tini -- /dock…"   About a minute ago   Up About a minute   0.0.0.0:8888->8081/tcp                           compose-mongo-express-1
0b4adbecfc32   busybox                                           "sh -c 'while true; …"   About a minute ago   Up About a minute                                                    my_busybox
c1303ade4ff2   confluentinc/cp-kafka:7.7.1                       "/etc/confluent/dock…"   About a minute ago   Up About a minute   0.0.0.0:9092->9092/tcp, 0.0.0.0:9101->9101/tcp   broker
a49dc3139cc9   postgres                                          "docker-entrypoint.s…"   About a minute ago   Up About a minute   0.0.0.0:5432->5432/tcp                           compose-db-1



4. Check if you can access kafka UI.
Kafka UI is served through docker container control-center. Check its port using command "docker ps". In our case its 9021. Access Kafka UI from your favorite browser by hitting http://localhost:9021



============================= Setting Up Postgres For CDC ==========================================================

1. Postgres installed from our compose file is running on port 5432.If you have manually installed postgres using exe its going to create conflict. Feel free to change the port in compose file for service db.

2. To enable CDC we need to modify the postgres.conf file and restart postgres. To access this file use the command
docker ps to find the busybox container. Check its name. In our case its my_busybox. 
Use following command to go inside busybox container
command: docker exec -it my_busybox /bin/sh

3. Change the following parameters in postgresql.conf  the values should be as follows.
The file can be found at /mnt/shared folder inside busybox container.

wal_level = logical
max_replication_slots = 4
max_wal_senders = 4

save the file and exit container.

command: exit

4. Restart Postgres container.
Use command docker ps to list the containers. Check the name of postgres container in our case it is compose-db-1.
Use following command to restart the postgres container
command: docker restart compose-db-1

5. Verify the settings are applied
Connect to postgres database using your favorite DB client tool. e.g. DBWeaver or PGAdmin.
Open SQL console and execute following commands and verify it shows the values same as we set in postgresql.conf file

show wal_level ;
show max_replication_slots;
show max_wal_senders;


Your Postgres is set now for CDC changes.



=================================== Install Kafka Debezium Connector for Postgres ============================================
Debezium Plugin for postgres needs to be installed in Kafka Connect container. 

1. Check the container name for connector using docker ps.In our case its connect.

2. Go inside the container for connect using the command

command: 
 docker exec -it connect /bin/bash

3. Switch to the directory 
/usr/share/confluent-hub-components

4. Down load the plugin by using following command

command : wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.9.7.Final/debezium-connector-postgres-1.9.7.Final-plugin.tar.gz

5. Extract the plugin once download is complete

command : tar -xvf debezium-connector-postgres-1.9.7.Final-plugin.tar.gz

6. Plugin installation is completed exit the container

command: exit

7. Restart the connect container

command: docker restart connect
Wait for couple of minutes till container for connect fully comes up. Verify the connector screen is loading from Kafka control center UI at http://localhost:9021 and click on Connect menu. 

8. Create a table in your postgres database. Using your SQL client (DBWeaver/PGAdmin) e.g. user_info. Any table you want to track using CDC feature we need to add it as publisher in Postgres. Fire below query to enable CDC logs for your table

CREATE PUBLICATION user_info_publication FOR TABLE public.user_info ;
ALTER TABLE public.user_info REPLICA IDENTITY FULL;

Note that public is schema name.Alter table query will ensure while publishing changes both before and after state is published.
Now your table is all set to emmit change logs.

9. Create a connector using postgres connector.
  Create a json file with following details,

   {
    "name": "Mongo_Connector",
    "config": {
        "name": "DataSync-Mongo_Connector",
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "tasks.max": "1",
        "key. converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "database.server.name": "mongosyncconfig",
        "database.hostname": "db",
        "database.port": "5432",
        "database.user": "postgres",
        "database.password": "example",
        "database.dbname": "user-management",
        "plugin.name": "pgoutput",
        "slot.name": "mongosyncconfig",
        "decimal.handling.mode": "double",
        "time.precision.mode": "adaptive",
        "snapshot.mode": "never",
        "table.include.list": "public.user_info"
    }
}

10. Import this json file in Kafka Connector screen
Click on Connect Menu on Left hand panel-->Click on connect-default --> Click on Add Connector button -- > Click on Upload connector config file

Give the path of Json file you created in #10 above.

Make sure you check the parameter in "Include Tables" text box. It should reflect the correct table name with schema in our case its public.user_info

11.Click on Next -- > Click on Launch

12. You will be redirected to Connectors page and you can see your connector in Error state first but within few seconds it should go in running state. Your connector is all set.

13.Go to topics you should see a topic with name matching to your database table name.In our case it would be something like mongosyncconfig.public.user_info. Check the messages in topic there should be one message which has table schema details.

14. Go to Postgres databse using DBWeaver/PGAdmin and add/modify any record in table. As soom as you modify the state of any record in table immediately kafka will receieve the message with changes you have made on topic mongosyncconfig.public.user_info.



=========================================== Mongo DB Setup ========================================================
1. Go inside mongo db container. Check the name of mongodb container using command docker ps.
In our case its compose-mongo-1 using this name get into the container using following command 

command: docker exec -it compose-mongo-1 mongosh -u root -p example --authenticationDatabase admin

2. Once inside mongo container create a new database, using following command

command: use audit-service-db

3. Create a collection inside audit-service-db

command: db.createCollection('user_info_audit')

3. Give root user read/write access to new DB you created inside mongodb by firing below command

command: 
1. use admin

2. db.grantRolesToUser("root", [{ role: "readWrite", db: "audit-service-db" }]);

4. Exit out of mongo container. command : exit

5. Check your DB in Mongo express UI by hitting URL in your browser http://localhost:8888

        



