Kafka Topics
	--> a perticular stream of data
	--> like table in a database(without all the constrains)
	--> kafka cluster
		--> logs
		--> purchases
		--> tweets
		--> truck_gps ..etc
	--> we can have as many topics as we want
	--> topic was identified by its name
	--> supports any kind of message format
	--> sequence of message called as data stream in topic
	--> we cant query the topics insted we use produces to send data and consumers to read data
	--> topics are splits in partitions
		--> messages with in partitions are orders(follows order)
	 	--> each message with in partitions gets an increment id called as offset
	 	--> offset only have meaning for a specific partitioin
	 	--> offsets are not reused even if previous message been deleted
	 	--> order is only guaranteed only with in a partition
	--> topics are immutable
	--> data kept in kafka in limited time default was 1 week can be configurable
	--> data is assigned randomly to partition unless akey provided

Producers and Message key
	--> producers write data to toopics (which are made of partitions)
	--> producer knows which partition to write to (and which kafka broker has it)
	--> In case of kafka broker failure, produces will autometically recover (this load is balanced to many brokers thanks to no of partitions)
	--> producers can choose to send a key with the message(string,number,binary ..etc) its optional
	--> if key is null then the data is sent round robin (part 0,then 1 ,then 2 ..)
	--> if key is not null then all messages for that key will always go to the same parttion(hashing)
	--> A key are typically sent if you need message ordering for specific field
	--> Kafka message created by the produce have below components
		--> Key -binary (can be null)
		--> Value - binary (can be null)
		--> Compression Type (none,gzip,snappy..etc)
		--> Headers (optional)
			--> key-value pairs
		--> Partition + Offset
		--> Timestamp(system or user set)
	--> kafka message serializer
		--> kakfka only accepts bytes as an input from produces and sends bytes out as an output to consumers
		--> message serialization means transforming objects/data into bytes
		--> they are used on the value and the key
		--> Common serializers
			--> String (incl. JSON)
			--> Int,Float
			--> Avro, Protobuf ..etc
	--> the default partitioner, the keys are hashed using the murmur2algorith

Consumers and deserialization
	--> Consumers read data from a topic(identified by name) - pull model
	--> Consumers automatically know which broker to read from
	--> in case of broker failures, consumers know how to recover
	--> Data is read in order from low to high offset within each partitions
	--> consumers will deserialize objects/data
	--> serialization/deserialization type must not change during a topic lifecycle(create new topic insted)
	--> Consumer groups
		--> all the consumers in an application read data as consumer groups
		--> each consumer within group reads from exclusive partitions
		--> if we have more consumers than partitions then some consumers will be inactive
		--> it is acceptable to have multiple consumer groups on the same topic (each goup may have different fuctinality like notification,location track ..etc)
		--> to create distinct consumer groups,use the consumer property group.id
	--> we can have multiple consumers on one topic
	--> Consumer offsets
		--> kafka stores the offsets at which a consumer group has been reading
		--> the offsets committed are in kafka topic named __consumer_offsets
		--> when a consumers in a group has processed data received from kafka,it should be periodically committing the offsets
		(kafka broker will write to __consumer_offsets, not the group itself)
		-->  if a consumer dies, it will be able to read back from where it left off thanks to the commited consumer offsets
	--> delivery semantics
		--> by default, java consumers will automatically commit offsets(atleast once)
		--> thare are 3 delivery semantics if you choose to commit manually
			--> atlease once(usually preferred)
				--> offsets are commited after message processed
				--> if processing going wrong, the message will be read again. it result in duplicate processing
				so we should make shure that process should be idempotent(i.e., processing again should not imapact system)
			--> At most once
				--> offsets are commited assoon as message received
				--> if the process gone wrong, some message can be lost
			--> Exactly once
				--> for kafka : kafka workflows : use the transactional API(easy with kafka streams API)
				--> for kafka : exyernal system workflows : use an idempotent consumer
Brokers and Topics
	--> A kafka cluster is composed of multiple brokers(servers)
	--> Each broker is identified with its ID (integer)
	--> Each broker contains certain topic partitions
	--> After connecting to any broker (called bootstrap broker), you will be connected to the entire cluster
	(kafka clients have smart machanics for that)
	--> A good number to get started is 3 brokers, but some big clusters have over 100 brokers
	--> Topic partitions will spread over the kafka brokers (horizantal scaling)
	--> every broker called as bootstrap server. we need to connect to one broker and kafka clients will know how to connected to entire cluster
	--> each brokers knows the meta data of all broker topics and its partitions

Topic Replication
	--> topics should have replication factor > 1(usually btw 2 and 3)
	--> if any broker down another broker serve the data
	--> at any time only one broker can be a leader for a given partition
	--> Producers can only send data to the broker that is leader of partition
	--> the other brokers will replcate the data. so each partition has one leader and multiple ISR (insync replication)
	--> by default kafka producers can only write to the broker for partition and consumer will read the data from leader
	--> since kafka 2.4, it is possible to configure consumers to read from the closest replica. this may reduce latency and also decrease network cost if using cloud
Producer Acknowledments(acks)
	-->produces can chosse to receive acknowledgement of data writes
		--> acks=o : producers wonnt wait for ack (possible of data loss)
		--> acks=1 : will wait for leader ack (limited data loss) default from 1.0 to 2.8
		--> acks=all : leader+ replica acks (no data loss) default for 3.0+
			--> we can set howmany replica acks needed for us to ack back using prop min.insync.replicas
				--> if min.insync.replicas =1 ; only broker leader needs to success ack
				--> if 2 atleasr the broker leader and one replica need to ack
Topic Durability
	--> For a topic replication factor of 3,topic data durability can withstand 2 brokers loss
	--> as a rule, for replication factor of N, you can permanantly lose up to N-1 brokers still recover data

Zookeeper
	--> it manages brokers
	--> it helps in performing leader election for partitions
	--> sends notifications to kafka in case of changes (ex: new topic, broker dies. broker up..etc)
	--> till 2.X kafka cant work without zookeeper
	--> kafka 3.x can work without zookeper(KIP-500) - using kafka Raft insted
	--> kafka 4.x will not have zookeeper
	--> by design zookeper operates with odd number of servers (1,3,5..)
	--> Zk has leader (writes) the rest of the servers are followers(reads)
	--> ZK doesnot store consumers offsets with kafka >v0.10
	--> should we use zookeper?
		--> with kafka brokers?
			--> yes untill kafka 4.x 
		--> with kafka clients?
			--> over the time , the kafka clients and CLI have been migrated to leverage the brokers as a connection endpoint insted of zK
			--> Since kafka 0.10, consumers stores offset in kafka and ZK and must not connect to ZK as it is deprecated
			--> Since kafka 2.2, the kafka-topics.sh CLI command referance kafka brokers and not ZK for topic managment(creation,deletion..)
			and the ZK CLI arguments is deprecated
			--> All the APIs and commands that were previously leveraging ZK are migrated to use kafka insted, so that when clusters are 
			migrated to be without Zk, the change is invisible to clients
			--> ZK is also less secure than kafka, so ZK ports should only to be opend to allow traffic from brokers and not the clients
			--> so to be great modern-day kafka developer, never ever use ZK as a conficuration in ur kafka clientsand other programs that connect to kafka
Kafka KRaft
	--> in 2020, apache kafka project started to work to remove the zookeeper depeendency from it (KIP-500)
	--> ZK shows scaling issues when kafka clusters have >100000 partitions
	--> by removing ZK, apache kafka can
		--> scale to millions of partitions, and become easier to maintain and setup
		--> improve stability, makes it easier to monotor, support and administation
		--> Single security model for the whole system
		--> Single process to start with kafka
		--> Faster controller shutdown and recovery time
		--> kafka 3.x now implements the Raft protocol(KRAFT) in order to replace ZK. Not production ready

Installation guide:
--> https://www.conduktor.io/kafka/how-to-install-apache-kafka-on-mac

Kafka CLI : 
--> use --bootstrap-server option everywhere not --zookeeper 
--> right : kafka-topics --bootstrap-server localhost:9092 
--> Wrong : kafka-topics --zookeeper localhost:2181 
--> kafka-topics --bootstrap-server localhost:9092 --list : list the topics 
--> kafka-topics --bootstrap-server localhost:9092 --create --topic my_topic : creation 
--> kafka-topics --bootstrap-server localhost:9092 --create --topic my_topic --partitions 3 --replication-factor 2 : with partition and refactor --> it will thow error if we have less brokers -->
--> kafka-topics --bootstrap-server localhost:9092 --describe --topic my_topic  : details of topic
--> kafka-topics --bootstrap-server localhost:9092 --delete  --topic my_topic : if winown it may crash
--> kafka-console-producer bootstrap-server localhost:9092 --topic my_topic : to brodue
	> entrt messages..etx
	> ctrl+c for terminate
--> kafka-topics --bootstrap-server localhost:9092 --delete  --topic my_topic : if winown it may crash
--> kafka-console-producer bootstrap-server localhost:9092 --topic my_topic --producer-property acks=all : set acks
	--> we can run same command even there is no existing topic it will create new topic for us
--> kafka-console-producer bootstrap-server localhost:9092 --topic my_topic  --property parse.key = true 
	--property key.separatoor=:   (for key value pair)
	> my key : my value
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic : empty result as the pointer willl be on last
	--> open new console and proce mesage so we can see same message can be seen in consumer terminal
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic --from-beginning : to read all messages
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic --from-beginning --formatter kafka.tools.DefaultMessageFormatter
	--property print.timestamp=true --property print.key=true --property print.value=true : to view in key-value pair
	if no key it will be null
--> kafka-console-consumer --bs-srv lh:9092 --topic my_topic --group my_consumer_group : to group consumers
	--> we can run same cod in different terminal to vire message distribution
--> kafka-consumer-group -bs-srv lh:9092 --group my_group --reset-offsets --to-erliest --topic my_topic
	--execute : to resetting offset till the beggining
--> kafka-consumer-group -bs-srv lh:9092 --group my_group --reset-offsets --shift-by -2 --topic my_topic
	--execute : to resetting offset to desired shift

Kafka_Java
	--> https://www.conduktor.io/kafka/kafka-sdk-list for java sdk download
	--> maven dependencies 
		--> java 11
		--> kafka-clients
		--> slf4j api
		--> slf4j simple better to keep both slfj same version
	--> producer
		--> create producer configuration
			  	Properties props= new Properties();
        		props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"127.0.0.1:9092");
        		props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        		props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());

		--> create the producer
			    KafkaProducer<String,String> produder = new KafkaProducer<String, String>(props);

  		--> Producer Record
        		ProducerRecord record = new ProducerRecord("demo","hi how are you");
        --> send data (async)
        		produder.send(record);
        --> send data (sync
       		 produder.flush();
        --> closing
        	produder.close();
    --> Producer with call back : callback method will be called dor each succcessfull send or exception

    	-->  while sending data
    		-->   produder.send(record, new Callback() {
            public void onCompletion(RecordMetadata recordMetadata, Exception e) {
                if(e== null)
                    logi.info("message send sucessfully");
                else
                    logi.info(e.getMessage());

            }
        });
    --> Producer with keys // partition will be preserved based key hash
    	--> ProducerRecord record = new ProducerRecord("demo","key1",hi how are you");
    --> Consumer
    	--> config  
    		Properties props= new Properties();
        	props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"127.0.0.1:9092");
        	props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        	props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());
        	props.setProperty(ConsumerConfig.GROUP_ID_CONFIG,"my_group_id");
        	props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest"); //"none/earliest/latest"
        --> create consumer
        	KafkaConsumer<String,String> consumer = new KafkaConsumer<String, String>(props);
        --> Topic sunscription
        	consumer.subscribe(Collections.singletonList("my_Topic")); // for single topic
        	consumer.subscribe(Arrays.asList("my_Topic_list")); // for multiple topics
        --> poll/read data

	        --> while (true){
	            ConsumerRecords<String,String> records = consumer.poll(Duration.ofMillis(1000));
	            for (ConsumerRecord<String,String> record:records) {
	                logi.info(record.key());
	                logi.info(record.value());}}
	    --> Gracefull shutdown
	    	--> after consumer creation
	    		--> get a reference to the current thread
	    			--> // shutdown thread
			        //get current thread

			        final Thread mainThread = Thread.currentThread();
			        //adding shutdown hook
			        Runtime.getRuntime().addShutdownHook(new Thread() {
			            public void run() {
			                logi.debug("Detected a shutdown. lets by calling wakeup function");
			                consumer.wakeup();
			                try {
			                    mainThread.join();
			                } catch (InterruptedException e) {
			                    e.printStackTrace();
			                }
			            }
			        });

			        try {

			            //topic subscription

			            consumer.subscribe(Collections.singletonList("my_Topic")); // for single topic
			            consumer.subscribe(Arrays.asList("my_Topic_list")); // for multiple topics

			            //poll/read data

			            while (true) {
			                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
			                for (ConsumerRecord<String, String> record : records) {
			                    logi.info(record.key());
			                    logi.info(record.value());
			                }

			            }
			        } catch (WakeupException we) {
			            we.getMessage()
			        } catch (Exception e) {
			            logi.error(e.getMessage());
			        } finally {

			            consumer.close(); // this will also do auto commit offset
			        }
		--> Consumer Groups
			--> groups will be relananceing on partitions on below statergies
				--> Eagar Rebalance (default)
					--> once new consumer adds. it will cancell existing membership and reasigning
					--> during short period of time, the entaire consumer group stop processing
					--> consumers dont necessarly get back the same partition previously assigned
				--> Cooerative rebalancing(incremental rebalance)
					--> reassignng a small subset of partions from one consumer to nother
					--> can go through several iterations
					--> Avoids "stop-the-world" events where all consumers stop processing
					--> property partition.assignment.strategy
						--> rangeAssignor : assign partitions on per-topic basis(can cause imbalance)
						--> RoundRobin
						--> StickyAssignor : balanced like RR and then minimises partition movements when consumer join/leave the group
						--> CooperativeStickyAssignor : rebalance strategy is identical to sticky assignor but suppors cooerative balance
						and therefore consumer can keep on consuming topic 
						--> RA and CSA are default
			--> if consumer leaves group and join back it will have new member ID. if we specify froup.instance.id makes consumer static member
				if its in session.timeout.ms
	--> Producer Retries
		--> in case of transient failures,developers are expected to handle exceptions,otherwise the data will be lost.
		--> Examples of failures
			--> NOT_ENOUGH_REPLICAS (due to min.insync.replicas settings)
		--> there is a retries settings
			--> default to 0 for kafka <=2.0
			--> defaults to 2147483647 for kafka >=2.1
			--> the retry.backoff.ms setting by default 100ms
		--> Timeouts
			--> if retries>0, for ex. its tries all retries then its bound to time out
			--> since 2.1 you can set delivery.timeout.ms=1200000==2 min 
	--> Idempotent Producer
		--> in kafka >=0.11 you can define a idempotent producer which wont introduce duplicates on network error
		--> it is greate to guarantee a stable and safe pipeline
		--> they are default since kafka 3.0 recommended to use them
		--> they comeup with 
			--> retries=Integer.MAX_VALUE
			--> max.in.flight.requests=1(kafka=0.11) where as 5 for >=1.0 for high performance
			--> acks= all
		--> we need to ser producerProps.put(enable.idempotence",true)
	--> Safe Producer
		--> since 3.0 the producer is safe by default
			--> acks = all (-1)
			--> enable.idempotence = true
		--> with >=2.8 default as 1 and false
		--> key props to make produce safe
			--> acks=all
			--> min.insync.replicas = 2(broker/topic level)
			--> enable.idempotent=true (duplicates are not introduced due to n/w retries)
			--> delivery.timeout.ms=120000
			--> max.in.flight.requests.per.connection=5
				--> ensure max performace while keeping message ordering
	--> Message copression at producer level
		--> produces usually send data that text-based, ex json
		--> in this case,it is to importance to compress to improve performance
		--> compression can be enabled at producer level doesnt require any config change in brokers or consumers
		--> compression.type can de none(default).gzip,lz2,snappy,zstd(kafka 2.1)
		--> we can set at broker level(all topics) or topic-level
		--> compression.type=prodcer(default),the broker takes compressed batch from procuder client 
		and write directly to topic log file withour recompressing the data. it was recommanded
		--> compression.type=none all batches decompressed by the broker
		--> compression.type= lz4(for ex)
			--> if its matching produer setting,data is stored on disk as is
			--> if not batches are decompressed by broker and then recompress ussing compression algoritm specified
	--> linger.ms and batch.size
		--> by default, producers try to send records asap
			--> it will have upto max.in.flight.... =5, meaning up to 5 message batches being in flight
			--> after this, if more msgs must be sent while others are in flight,kafka is smart and will start batching them before the next batch send
		--> above two settings to influence the batching machanism
		--> linger.ms:default 0 how long to wait untill we send a batch. adding small number ex 5 helps add more msgs in that atch at expence of latency
		--> batch-size: default 16KB if batch is filled before linger.ms increase batch size
		       	props.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG,"snappy");
        		props.setProperty(ProducerConfig.LINGER_MS_CONFIG,"20");
        		props.setProperty(ProducerConfig.BATCH_SIZE_CONFIG,Integer.toString(32*1024));
    --> if the producer produces faster than the broker , the records will be buffered in memory. below are helpfull
    	--> max.block.ms = 60000
    	--> bugger.memory = 33554432(32 MB) the size of send buffer

    --> Consumers (read data from topic send it to opensearch)
    	--> Delivery Semantics
    		--> Atmost once : offsets are committed as soon as the message batch received.if process gone wrong it wont read again
    		--> At least Once(preffered) : offsets are commited after msg processed. if process gone wrong it will read again. can be duplicate processing
    			so we need to make sure process in idempotent
    		--> Exactly once : can be acieved for kafka => kafka workflows using transactional API
    			(easy with kafka Streams API). for kafka => sink workflows,use an idempotent consumer
    		-->  we can make comsumer process to make idempotent ase on the ID
    	--> Offset Commit strategies
    		--> (easy) enable.auto.commit = true & synchroneous processing of batches
    			--> in the java consumer API, offsets are regularly commites
    			--> enable at-least once reading senario by default
    			--> offsets are commited when you call .poll() and auto.commit.interval.ms has elapsed
    			--> make sure msgs are all successfully processed before you call .poll() again
    				if we dont,you will not be in at-lease-once reading scenario
    			--> in that(rare) case, you must diable auto commit, and most likely most processing to a separate thread and then from time-to-time call
    				.commitSync() or .commitAsync() with correct offsets manually(advanced)
    			--> default intervel 5000
    		--> (medium) enable.auto.commit = false and manual commit of offset
    		--> Offset reset behaviour
    			--> A consumer expected to read from log continuosly
    			--> but if your application has bug, consumer will go down
    			--> if kafka has retention of 7 days, your consumer is down for more than 7 offsets are invalid
    			--> auto.offset.reset 
    				--> latest : will read from end of log
    				--> earlier : from start
    				--> none : throw exception if no offset found
    			--> offsets can be lost if 
    				--> consumer hasnt read data in 1 day (kafka < 2.0) where as 7 days for >=2.0
    			--> can be controlled by broker settings offset.retention.minutes
    	--> Controlling Consumer liveliness
    		--> Consumers in a group talk to a consumer groups coordinator
    		--> to detect consumers that are down there is a heartbeat mechanism and a poll machanism
    		--> to avoid issues,consumers are encouraged to process data fast and poll ofen
    		--> heartbeat.interval.ms (default 3 sec) usually set to 1/3rd of session.time.out.ms
    		--> session.timeout.ms (default 45 sec kafka 3.0+ before 10 sec)
    		--> heartbeats are sent periodically to the broker
    		--> if no heartbeat is sent during that period, the consumer is considered 
    		--> set even lower to faster consumer rebalanes
    		--> max.poll.interval.ms(default 5 mins)
    			--> max amount of time btw two .poll() calls before declaring the consumer dead
    			--> this machanism is used to detect a data processiing issue with the consumer(consumer is stuck)
    			--> max.poll.records(default 500)
    				--> controls how many records to receive per poll request
    				--> increase if msg are very small and have a lot of available RAM
    			--> fetch.min.bytes (default 1)
    				--> controls how much data we want to pull at least on each request
    			--> fetch.max.wait.ms(default 500)
    				--> the max amount of time the broker will block before answering
    					the fetch request if there isnt sufficient data to immediately satisfy the requirement given by fetch.min.bytes
    	--> default consumer behaviour with partition leaders
    		--> by default will read from the leader broker for a partition
    		--> possibly higher latency (multiple data centre)+ high network chargers 
    		--> ex: Datacentre == Availability Zone(AZ) in AWS 
    		--> since 2.4 it is possible to configure consumers to read from the closest replica
    		--> broker settings(must ve 2.4+)
    			--> rack.id  config must be set to data centreID(ex AZ ID in AWS) : rack.id=usw2-az1
    			--> replica.selector.class must be set to org.apache.kafka.common.replica.RackAwareReplicaSelector
    		--> Consmer settings
    			--> client.rack to the data centre ID the consumer is launched on
    			 



        



